{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import polars as pl\n",
    "import inspect\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import dsds.metrics as me\n",
    "import dsds.prescreen as ps\n",
    "import dsds.sample as sa\n",
    "import dsds.fs as fs\n",
    "import dsds.transform as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../data/train.csv\")\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsds._rust import rs_tfidf_vectorizer, rs_ref_table\n",
    "\n",
    "res = rs_tfidf_vectorizer(df, \"Description\", \"snowball\", 0.02, 0.95, 5000, 500, True)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "res = rs_tfidf_vectorizer(df, \"Description\", \"snowball\", 0.02, 0.95, 5000, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"Description\"\n",
    "df = df.with_columns(\n",
    "    pl.col(c).str.to_lowercase()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_ref_table(df, \"Description\", \"snowball\", 0.02, 0.95, 5000, 500).sort(\"ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsds.text as tt\n",
    "res = tt.tfidf_vectorizer(df, \"Description\", \"snowball\", 0.02, 0.95, 5000, 500, True)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "res = tt.tfidf_vectorizer(df, \"Description\", \"snowball\", 0.02, 0.95, 5000, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsds._rust import rs_cnt_vectorizer, rs_ref_table, rs_snowball_stem, rs_levenshtein_dist\n",
    "import dsds.text as tt\n",
    "c = \"Ad Topic Line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "corpus = df[\"Description\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.02)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "cols = vectorizer.get_feature_names_out().tolist()\n",
    "df_words = pl.from_numpy(X, schema=cols)\n",
    "df_combined = pl.concat([df, df_words], how=\"horizontal\")\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "corpus = ['this is the first document', 'this document is the second document', 'and this is the third one','is this the first document']\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the','and', 'one']\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)), ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "pipe['tfid'].idf_\n",
    "pipe.transform(corpus).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.transform(corpus).toarray() # .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[\"count\"].get_feature_names_out().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../data/advertising.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsds.metrics as me\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def pure_numpy_logloss(y_actual:np.ndarray, y_predict:np.ndarray):\n",
    "    return -np.mean(y_actual * np.log(y_predict) + (1 - y_actual) * np.log(1 - y_predict))\n",
    "\n",
    "pred = np.random.random(size=500_000) # Some random fake predictions\n",
    "actual = np.round(np.random.random(size=500_000)).astype(np.int8) # Some random fake actual labels\n",
    "# Yielding the same result up to 12 digits\n",
    "print(round(me.logloss(actual, pred), 12) == round(log_loss(actual, pred), 12))\n",
    "print(round(me.logloss(actual, pred), 12) == round(pure_numpy_logloss(actual, pred), 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "me.logloss(actual, pred) # dsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "log_loss(actual, pred) # sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pure_numpy_logloss(actual, pred) # pure numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# orig_x, orig_y = make_classification(n_samples = 100_000, n_features = 10, n_informative = 5, n_redundant = 5)\n",
    "# # This is a Polars dataframe. This is dsds package's favored dataframe. dsds relies on Polars heavily.\n",
    "# # You must turn other dataframe formats into Polars for dsds to work.\n",
    "# df = pl.from_numpy(orig_x).insert_at_idx(0, pl.Series(\"target\", orig_y)) \n",
    "# # Turn it into Pandas.\n",
    "# df_pd = df.to_pandas()\n",
    "# target = \"target\"\n",
    "# features = df.columns\n",
    "# features.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame({\n",
    "    \"a\": [1, np.nan, None],\n",
    "    \"b\": [1,2,3]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.invalid_inferral(df, threshold=0.5, include_null=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.discrete_inferral(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ps.get_unique_count(df.with_row_count(offset=1).set_sorted(\"row_nr\"), include_null_count=True)\n",
    "len_df = temp.filter(pl.col(\"column\") == \"row_nr\").item(0,1)\n",
    "print(len_df)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "orig_x, orig_y = make_classification(n_samples = 50_000, n_features = 50, n_informative = 20, n_redundant = 30)\n",
    "df_pl = pl.from_numpy(orig_x).insert_at_idx(0, pl.Series(\"target\", orig_y))\n",
    "features = df_pl.columns\n",
    "features.remove(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in sa.time_series_split(df_pl, n_splits=5, offset = 1000):\n",
    "    print(train.shape)\n",
    "    print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fs.discrete_ig(df_pl, target=\"target\", cols=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fs.discrete_ig2(df_pl, target=\"target\", cols=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "me.logloss(actual, pred, check_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
