{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from time import perf_counter\n",
    "\n",
    "import dsds.transform as t\n",
    "import dsds.fs as fs\n",
    "\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_x, orig_y = make_classification(n_samples = 100_000, n_features = 100, n_informative = 60, n_redundant = 40)\n",
    "# This is a Polars dataframe. This is dsds package's favored dataframe. dsds relies on Polars heavily.\n",
    "# You must turn other dataframe formats into Polars for dsds to work.\n",
    "df = pl.from_numpy(orig_x).insert_at_idx(0, pl.Series(\"target\", orig_y)) \n",
    "# Turn it into Pandas.\n",
    "df_pd = df.to_pandas()\n",
    "target = \"target\"\n",
    "features = df.columns\n",
    "features.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "This notebook compares results and performance between the dsds package, sklearn and some other packages for feature selection and some other transformations common in the data science pipeline.\n",
    "\n",
    "### Methods Compared:\n",
    "1. Fscore\n",
    "2. Mutual Information Score\n",
    "3. MRMR feature selection strategies\n",
    "4. Power Transform\n",
    "\n",
    "You may restart the kernel after each section. But remember to rerun the cells above. If you are concerned about memory usage when running this notebook, go to the end and run the gc cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "res = fs.f_classif(df, target=target)\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing Fscore.\")\n",
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "f, pv = f_classif(df_pd[features], df_pd[target])\n",
    "res = pd.DataFrame({\"feature\":features, \"f_value\":f, \"p_value\":pv})\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing Fscore.\")\n",
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "f, pv = f_regression(df_pd[features], df_pd[target])\n",
    "res = pd.DataFrame({\"feature\":features, \"f_value\":f, \"p_value\":pv})\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start: .2f}s in computing Fscore.\")\n",
    "res.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took 16s\n",
    "fs.mutual_info(df, target=target, conti_cols=features).sort(by=\"estimated_mi\", descending=True).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mi_sklearn(df:pd.DataFrame, cols:list[str], target:str, k=3, random_state:int=42):\n",
    "    mi_estimates = mutual_info_classif(df[cols], df[target]\n",
    "                        , n_neighbors=k, random_state=random_state, discrete_features=False)\n",
    "\n",
    "    return pl.from_records([cols, mi_estimates], schema=[\"feature\", \"estimated_mi\"]).sort(\"estimated_mi\", descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took 1m38s seconds by using sklearn. The reason is that sklearn's implementation did not turn on multithreading for KDtrees.\n",
    "# Sklearn also did not provide an option to turn it on, despite the fact that sklearn's KDtrees\n",
    "# does have this functionality.\n",
    "estimate_mi_sklearn(df_pd, cols=features, target=target).limit(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRMR Feature selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif # This is currently the most starred MRMR Python package on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to wrap it so that we get apples to apples comparison\n",
    "def mrmr_package(df:pd.DataFrame, target:str, k:int) -> list[str]:\n",
    "    features = list(df.columns)\n",
    "    features.remove(target)\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    start = perf_counter()\n",
    "    output = mrmr_classif(X, y, K = k)\n",
    "    end = perf_counter()\n",
    "    print(f\"Spent {end - start:.2f}s to compute mrmr.\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrmr_package(df_pd, \"target\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "res = fs.mrmr(df, target=\"target\", k = 50, low_memory=False)\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "res = fs.mrmr(df, target=\"target\", k = 50, low_memory=True)\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager transform.\n",
    "start = perf_counter()\n",
    "res_eager = t.power_transform(df, target = target, strategy=\"yeo_johnson\")\n",
    "end = perf_counter()\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")\n",
    "res_eager.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn with Pandas\n",
    "\n",
    "# In this case, the benefits of using dsds are:\n",
    "# 1. Much better performance.\n",
    "# 2. Users need to do extra steps to put data back in dataframe format.\n",
    "# 3. Dsds can save the plan using LazyFrame.blueprint.preserve_as_pickle()... But in Pandas,\n",
    "# you have to use the PowerTransformer class, which makes code longer and uglier (and less organized)\n",
    "\n",
    "start = perf_counter()\n",
    "transformed = power_transform(df_pd[features], method = \"yeo-johnson\", standardize=False)\n",
    "end = perf_counter()\n",
    "df_pd[features] = transformed\n",
    "print(f\"Spent {end - start:.2f}s in computing.\")\n",
    "df_pd.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
